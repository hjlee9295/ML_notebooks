{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Machine Learning HW 4\n",
    "\n",
    "#### Hojin Lee (hl3328) & Hyuk Joon Kwon (hk3084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv file\n",
    "wine_df_raw = pd.read_csv(r'wine-reviews/winemag-data-130k-v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129971 entries, 0 to 129970\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Unnamed: 0             129971 non-null  int64  \n",
      " 1   country                129908 non-null  object \n",
      " 2   description            129971 non-null  object \n",
      " 3   designation            92506 non-null   object \n",
      " 4   points                 129971 non-null  int64  \n",
      " 5   price                  120975 non-null  float64\n",
      " 6   province               129908 non-null  object \n",
      " 7   region_1               108724 non-null  object \n",
      " 8   region_2               50511 non-null   object \n",
      " 9   taster_name            103727 non-null  object \n",
      " 10  taster_twitter_handle  98758 non-null   object \n",
      " 11  title                  129971 non-null  object \n",
      " 12  variety                129970 non-null  object \n",
      " 13  winery                 129971 non-null  object \n",
      "dtypes: float64(1), int64(2), object(11)\n",
      "memory usage: 13.9+ MB\n"
     ]
    }
   ],
   "source": [
    "wine_df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 54504 entries, 2 to 129967\n",
      "Data columns (total 14 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             54504 non-null  int64  \n",
      " 1   country                54504 non-null  object \n",
      " 2   description            54504 non-null  object \n",
      " 3   designation            36908 non-null  object \n",
      " 4   points                 54504 non-null  int64  \n",
      " 5   price                  54265 non-null  float64\n",
      " 6   province               54504 non-null  object \n",
      " 7   region_1               54226 non-null  object \n",
      " 8   region_2               50511 non-null  object \n",
      " 9   taster_name            37730 non-null  object \n",
      " 10  taster_twitter_handle  34741 non-null  object \n",
      " 11  title                  54504 non-null  object \n",
      " 12  variety                54504 non-null  object \n",
      " 13  winery                 54504 non-null  object \n",
      "dtypes: float64(1), int64(2), object(11)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#leaving all other countries but US\n",
    "df_us = wine_df_raw[wine_df_raw['country'] == 'US']\n",
    "df_us.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping null values & setting sample size as 50000\n",
    "dropna = True\n",
    "sample_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical feature: ['country', 'description', 'designation', 'province', 'region_1', 'region_2', 'taster_name', 'taster_twitter_handle', 'title', 'variety', 'winery']\n",
      "Continuous feature: ['Unnamed: 0', 'points', 'price']\n"
     ]
    }
   ],
   "source": [
    "print('Categorical feature:',list(df_us.select_dtypes(include=['object']).columns))\n",
    "print('Continuous feature:',list(df_us.select_dtypes(exclude=['object']).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country 1\n",
      "description 50449\n",
      "designation 14184\n",
      "province 27\n",
      "region_1 265\n",
      "region_2 18\n",
      "taster_name 16\n",
      "taster_twitter_handle 13\n",
      "title 50229\n",
      "variety 257\n",
      "winery 5375\n"
     ]
    }
   ],
   "source": [
    "#printing the number of unique values for each feature\n",
    "for col in list(df_us.select_dtypes(include=['object']).columns):\n",
    "    print(col, len(df_us[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 Create a baseline model for predicting wine quality using only non-text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping couple features. Unnamed: 0 is a meaningless feature. Country is not helpful as we only filtered US.\n",
    "# taster_twitter_handle will have high collinearlity with taster_name. Province and region_2 will have high \n",
    "# collinearilty with region_1.\n",
    "# title and description is almost unique identifier.\n",
    "drop_cols = ['Unnamed: 0','country','taster_twitter_handle', 'province', 'region_2', 'title', 'description']\n",
    "\n",
    "df_drop = df_us.drop(drop_cols, axis=1)\n",
    "\n",
    "if dropna:\n",
    "    df = df_drop.dropna(subset=['points'])\n",
    "else:\n",
    "    df = df_drop\n",
    "\n",
    "df_sample = df.sample(n=sample_size, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample.loc[:, df_sample.columns != 'points']\n",
    "y = df_sample['points']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = X_train.dtypes == object\n",
    "\n",
    "cat_preprocessing = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='NaN'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "cont_preprocessing = make_pipeline(\n",
    "    SimpleImputer())\n",
    "\n",
    "cont_preprocessing_scale = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    StandardScaler())\n",
    "\n",
    "target_encoder = make_pipeline(\n",
    "    ce.TargetEncoder()\n",
    "    , StandardScaler())\n",
    "\n",
    "te_feature = ['designation','winery']\n",
    "cont_feature = list(X_train.select_dtypes(exclude=['object']).columns)\n",
    "cat_feature = list(X_train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (cont_preprocessing, cont_feature)\n",
    "    , (cat_preprocessing, cat_feature)\n",
    "    , remainder ='passthrough')\n",
    "\n",
    "preprocess_scale = make_column_transformer(\n",
    "    (cont_preprocessing_scale, cont_feature)\n",
    "    , (cat_preprocessing, cat_feature)\n",
    "    , remainder ='passthrough')\n",
    "\n",
    "cat_feature = list(set(list(X_train.select_dtypes(include=['object']).columns)) - set(te_feature))\n",
    "preprocess_scale_te = make_column_transformer(\n",
    "    (target_encoder, te_feature)\n",
    "    , (cont_preprocessing_scale, cont_feature)\n",
    "    , (cat_preprocessing, cat_feature)\n",
    "    , remainder ='passthrough')\n",
    "\n",
    "def pipeline_prediction(X, y, preprocess, regression):\n",
    "    OLR_pipe = make_pipeline(preprocess, regression)\n",
    "    scores_olr = cross_val_score(OLR_pipe, X, y, cv=5)\n",
    "    return np.mean(scores_olr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------------+-------------------------------+\n",
      "|                   |   preprocess |   preprocess_with_scaler |   preprocess_with_scaler & te |\n",
      "|-------------------+--------------+--------------------------+-------------------------------|\n",
      "| Linear_regression |     0.33972  |                 0.33963  |                      0.446684 |\n",
      "| Ridge             |     0.444749 |                 0.462038 |                      0.448081 |\n",
      "+-------------------+--------------+--------------------------+-------------------------------+\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "methods = [LinearRegression(), Ridge()]\n",
    "processors = [preprocess, preprocess_scale, preprocess_scale_te]\n",
    "method_name = ['Linear_regression', 'Ridge']\n",
    "processors_name = ['preprocess','preprocess_with_scaler','preprocess_with_scaler & te']\n",
    "\n",
    "processor_counter = 0\n",
    "\n",
    "for processor in processors:\n",
    "    method_counter = 0\n",
    "    results_dict[processors_name[processor_counter]] = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        \n",
    "        results_dict[processors_name[processor_counter]][method_name[method_counter]] = pipeline_prediction(X_train, y_train, processor, method)\n",
    "        method_counter += 1\n",
    "        \n",
    "    processor_counter += 1\n",
    "        \n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Create a simple text-based model using a bag-of-words approach and a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainval, y_trainval = df_us['description'], df_us['points']\n",
    "text_train, text_val, y_train, y_val = train_test_split(text_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(token_pattern=r\"\\b\\w+\\b\", max_features = 5000)\n",
    "X_train = vect.fit_transform(text_train)\n",
    "X_val = vect.transform(text_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|                   |   Count Vectorizer |\n",
      "|-------------------+--------------------|\n",
      "| Linear_regression |           0.692312 |\n",
      "| Ridge             |           0.69573  |\n",
      "+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "methods = [LinearRegression(), Ridge()]\n",
    "processors = [CountVectorizer]\n",
    "method_name = ['Linear_regression', 'Ridge']\n",
    "processors_name = ['Count Vectorizer']\n",
    "\n",
    "processor_counter = 0\n",
    "\n",
    "for processor in processors:\n",
    "    method_counter = 0\n",
    "    results_dict[processors_name[processor_counter]] = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        \n",
    "        results_dict[processors_name[processor_counter]][method_name[method_counter]] = method.fit(X_train, y_train).score(X_val, y_val)\n",
    "        method_counter += 1\n",
    "        \n",
    "    processor_counter += 1\n",
    "        \n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 Try using n-grams, characters, tf-idf rescaling and possibly other ways to tune the BoW model. Be aware that you might need to adjust the (regularization of the) linear model for different feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_us.sample(10000)\n",
    "text_trainval, y_trainval = df_sample['description'], df_sample['points']\n",
    "text_train, text_val, y_train, y_val = train_test_split(text_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.6256811727204594\n",
      "best param:  {'countvectorizer__max_features': 5000, 'countvectorizer__min_df': 1, 'countvectorizer__ngram_range': (1, 1), 'normalizer': None, 'ridge__alpha': 10}\n",
      "--- 1465.7394850254059 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#n-grams, character, regression, grid search\n",
    "param_grid = {\n",
    "    \"countvectorizer__ngram_range\":[(1,1),(1,3),(1,4)]\n",
    "    , \"countvectorizer__min_df\":[1,3,5]\n",
    "    , \"ridge__alpha\": [100, 10, 1, 0.1, 0.01]\n",
    "    , \"countvectorizer__max_features\": [1000, 3000, 5000]\n",
    "    , \"normalizer\": [None, Normalizer()]\n",
    "    \n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    make_pipeline(CountVectorizer(stop_words='english'), Normalizer(), Ridge()), param_grid=param_grid, cv=5\n",
    ")\n",
    "\n",
    "grid.fit(text_train, y_train)\n",
    "print('best score: ',grid.best_score_)\n",
    "print('best param: ',grid.best_params_)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score:  0.6029701521223958\n",
      "best param:  {'normalizer': Normalizer(copy=True, norm='l2'), 'ridge__alpha': 1, 'tfidfvectorizer__max_features': 5000, 'tfidfvectorizer__min_df': 3, 'tfidfvectorizer__ngram_range': (1, 1)}\n",
      "--- 963.1393620967865 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#tf-idf rescaling\n",
    "start_time = time.time()\n",
    "param_grid = {\n",
    "    \"tfidfvectorizer__ngram_range\": [(1,1), (1,3), (1,4)]\n",
    "    , \"tfidfvectorizer__min_df\": [1, 3, 5]\n",
    "    , \"ridge__alpha\": [100, 10, 1, 0.1, 0.01]\n",
    "    , \"tfidfvectorizer__max_features\": [3000,5000]\n",
    "    , \"normalizer\": [None, Normalizer()]   \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    make_pipeline(TfidfVectorizer(stop_words='english'), Normalizer(), Ridge()), param_grid=param_grid, cv=5\n",
    "\n",
    ")\n",
    "grid.fit(text_train, y_train)\n",
    "print('best score: ',grid.best_score_)\n",
    "print('best param: ',grid.best_params_)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Combine the non-text features and the text features. How does adding those features improve upon just using bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Unnamed: 0','country','taster_twitter_handle', 'province', 'region_2', 'title']\n",
    "\n",
    "df_drop = df_us.drop(drop_cols, axis=1)\n",
    "\n",
    "X = df_drop.loc[:, df_drop.columns != 'points']\n",
    "y = df_drop['points']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessing = make_pipeline(\n",
    "    CountVectorizer(max_features = 5000, min_df=1, ngram_range=(1,1), stop_words='english')\n",
    "    )\n",
    "\n",
    "te_feature = ['designation','winery']\n",
    "text_feature = ['description']\n",
    "cont_feature = list(X_train.select_dtypes(exclude=['object']).columns)\n",
    "cat_feature = list(set(list(X_train.select_dtypes(include=['object']).columns)) - set(te_feature) - set(text_feature))\n",
    "\n",
    "preprocess_text = make_column_transformer(\n",
    "    (target_encoder, te_feature)\n",
    "    , (cont_preprocessing_scale, cont_feature)\n",
    "    , (cat_preprocessing, cat_feature)\n",
    "    , (text_preprocessing, 'description')\n",
    "    , remainder ='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+\n",
      "|                   |   preprocess_with_Text |\n",
      "|-------------------+------------------------|\n",
      "| Linear_regression |               0.753456 |\n",
      "| Ridge             |               0.757582 |\n",
      "+-------------------+------------------------+\n"
     ]
    }
   ],
   "source": [
    "results_dict= {}\n",
    "methods = [LinearRegression(), Ridge(alpha=10)]\n",
    "processors = [preprocess_text]\n",
    "\n",
    "method_name = ['Linear_regression', 'Ridge']\n",
    "processors_name = ['preprocess_with_Text']\n",
    "\n",
    "processor_counter = 0\n",
    "\n",
    "for processor in processors:\n",
    "    method_counter = 0\n",
    "    results_dict[processors_name[processor_counter]] = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        \n",
    "        results_dict[processors_name[processor_counter]][method_name[method_counter]] = pipeline_prediction(X_train, y_train, processor, method)\n",
    "        method_counter += 1\n",
    "        \n",
    "    processor_counter += 1\n",
    "        \n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pretrained word-embedding (word2vec, glove or fasttext) for featurization instead of the bag-of-words model. Does this improve classification? How about combining the embedded words with the BoW model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainval, y_trainval = df_us['description'], df_us['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainval = text_trainval.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainval = text_trainval.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))\n",
    "text_trainval = text_trainval.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp.pipe(text_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector = np.array([text.vector for text in document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X.reset_index(drop=True)\n",
    "X_ = pd.concat([X_, pd.DataFrame(text_vector)], axis=1)\n",
    "\n",
    "te = ce.TargetEncoder(cols=['designation','winery',]).fit(X_, y_trainval)\n",
    "X_ = te.transform(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, y_train, y_val = train_test_split(X_, y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = ['description']\n",
    "cont_feature = ['price', 'designation','winery']\n",
    "cat_feature = ['region_1','taster_name', 'variety']\n",
    "\n",
    "preprocess_text = make_column_transformer(\n",
    "    (cont_preprocessing_scale, cont_feature)\n",
    "   , (cat_preprocessing, cat_feature)\n",
    "   , (text_preprocessing, 'description')\n",
    "   , remainder ='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                   |   preprocess_text |\n",
      "|-------------------+-------------------|\n",
      "| Linear_regression |          0.747052 |\n",
      "| Ridge             |          0.75558  |\n",
      "+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "methods = [LinearRegression(), Ridge(alpha=10)]\n",
    "processors = [preprocess_text]\n",
    "\n",
    "method_name = ['Linear_regression', 'Ridge']\n",
    "processors_name = ['preprocess_text']\n",
    "\n",
    "processor_counter = 0\n",
    "\n",
    "for processor in processors:\n",
    "    method_counter = 0\n",
    "    results_dict[processors_name[processor_counter]] = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        \n",
    "        results_dict[processors_name[processor_counter]][method_name[method_counter]] = pipeline_prediction(text_train, y_train, processor, method)\n",
    "        method_counter += 1\n",
    "        \n",
    "    processor_counter += 1\n",
    "        \n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='psql'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
